cd docker &&
./build_docker_amd64.sh //

./create_container_amd64.sh //

docker exec -it dyno_sam bash

export MAKEFLAGS="-j10" && clear && colcon build


---- ros package
cd src

ros2 pkg create rgbd_publisher_pkg \
  --build-type ament_python \
  --dependencies rclpy sensor_msgs cv_bridge

cd ..

nano src/rgbd_publisher_pkg/setup.py

--

from setuptools import setup

package_name = 'rgbd_publisher_pkg'

setup(
    name=package_name,
    version='0.0.0',
    packages=[package_name],
    install_requires=['setuptools'],
    zip_safe=True,
    maintainer='user',
    maintainer_email='user@todo.todo',
    description='RGBD publisher for DynoSAM',
    license='TODO',
    tests_require=['pytest'],
    entry_points={
        'console_scripts': [
            'rgbd_publisher = rgbd_publisher_pkg.rgbd_publisher:main',
        ],
    },
)
--
nano src/rgbd_publisher_pkg/rgbd_publisher_pkg/rgbd_publisher.py

--
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
import cv2
import glob
import yaml
import numpy as np
import os

class RGBDPublisher(Node):

    def __init__(self):
        super().__init__('rgbd_publisher')

        # Topics match what DynoSAM expects
        self.rgb_pub = self.create_publisher(Image, '/camera/color/image_rect_color', 10)
        self.depth_pub = self.create_publisher(Image, '/camera/aligned_depth_to_color/image_raw', 10)
        self.info_pub = self.create_publisher(CameraInfo, '/camera/color/camera_info', 10)

        self.bridge = CvBridge()

        # Data Paths
        self.data_dir = '/root/data'
        self.rgb_files = sorted(glob.glob(os.path.join(self.data_dir, 'rgb', '*.png')))
        self.depth_files = sorted(glob.glob(os.path.join(self.data_dir, 'depth', '*.png')))
        
        if not self.rgb_files:
            self.get_logger().error(f"No RGB images found in {self.data_dir}/rgb")
        
        self.cam_info = self.load_camera_info(os.path.join(self.data_dir, 'camera_info.yaml'))

        self.idx = 0
        # 30 FPS (0.033s)
        self.timer = self.create_timer(0.033, self.publish_frame)
        self.get_logger().info("RGBD Publisher Started!")

    def load_camera_info(self, path):
        msg = CameraInfo()
        try:
            with open(path) as f:
                data = yaml.safe_load(f)
            msg.width = data.get('image_width', 640)
            msg.height = data.get('image_height', 480)
            msg.k = data.get('camera_matrix', {}).get('data', [])
            msg.d = data.get('distortion_coefficients', {}).get('data', [])
            msg.r = data.get('rectification_matrix', {}).get('data', [])
            msg.p = data.get('projection_matrix', {}).get('data', [])
            msg.distortion_model = data.get('distortion_model', 'plumb_bob')
        except Exception as e:
            self.get_logger().warn(f"Could not load camera info: {e}. Using empty default.")
        return msg

    def publish_frame(self):
        if not self.rgb_files: return

        # Loop functionality: Restart if we hit the end
        if self.idx >= len(self.rgb_files):
            self.idx = 0
            self.get_logger().info("Looping dataset...")

        # Read Images
        rgb = cv2.imread(self.rgb_files[self.idx])
        # Read depth as-is (-1)
        depth = cv2.imread(self.depth_files[self.idx], -1)

        if rgb is None or depth is None:
            self.get_logger().warn(f"Failed to read frame {self.idx}")
            self.idx += 1
            return

        # --- DEPTH CONVERSION CRITICAL STEP ---
        # DynoSAM expects 16-bit unsigned (millimeters). 
        # If your input is 8-bit (0-255), we must cast it or SLAM will fail.
        if depth.dtype != np.uint16:
            # Assuming input 0-255 map where 255 is roughly 5 meters?
            # This is a guess. Ideally, generate real 16-bit depth.
            depth = depth.astype(np.uint16) * 20 # Scale factor hack if needed
        
        # Convert to ROS messages
        rgb_msg = self.bridge.cv2_to_imgmsg(rgb, encoding='bgr8')
        depth_msg = self.bridge.cv2_to_imgmsg(depth, encoding='16UC1')

        # Sync Timestamps
        now = self.get_clock().now().to_msg()
        rgb_msg.header.stamp = now
        rgb_msg.header.frame_id = "camera_optical_frame"
        depth_msg.header.stamp = now
        depth_msg.header.frame_id = "camera_optical_frame"
        self.cam_info.header.stamp = now
        self.cam_info.header.frame_id = "camera_optical_frame"

        # Publish
        self.rgb_pub.publish(rgb_msg)
        self.depth_pub.publish(depth_msg)
        self.info_pub.publish(self.cam_info)

        self.idx += 1

def main():
    rclpy.init()
    node = RGBDPublisher()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()


--
rm -rf /home/user/dev_ws/build/dynosam_nn/ament_cmake_python/dynosam_nn_py/dynosam_nn_py
colcon build --packages-select dynosam_nn --symlink-install --allow-overriding dynosam_nn

source /home/user/dev_ws/install/setup.bash
python3 -c "import dynosam_nn_py._core; print('ok')"

--

python3 -m pip cache remove "tensorrt*"
python3 -m pip install --upgrade tensorrt tensorrt-lean tensorrt-dispatch


dpkg -l | grep TensorRT
---

# in home/user/dev_ws
colcon build --packages-select rgbd_publisher_pkg

source install/setup.bash

ros2 run rgbd_publisher_pkg rgbd_publisher




----

source install/setup.bash


ros2 launch dynosam_ros dyno_sam_launch.py \
  params_path:=/home/user/dev_ws/src/core/dynosam/params/ \
  dataset_path:=/root/data/ \
  v:=1 \
  data_provider_type:=3 \
  output_path:=/root/results


-------------------------

ros2 launch dynosam_ros dyno_sam_online_rgbd_launch.py \
    dataset_path:=/root/data \
    output_path:=/root/results \
    name:=test \
    data_provider_type:=3 \
    run_pipeline:=true


ros2 launch dynosam_ros dyno_sam_online_rgbd_launch.py \
    online:=true \
    input_image_mode:=1 \
    dataset_path:=/root/data \
    params_path:=/home/user/dev_ws/src/core/dynosam/params/ \
    output_path:=/root/results \
    name:=test

---

docker run -it --gpus all --net=host \
-v ~/fish_data:/root/data \
-v ~/results:/root/results \
dyno_sam bash

# First, source the ROS setup (standard practice inside these containers)
source /opt/ros/humble/setup.bash  # or foxy/galactic depending on the image
source install/setup.bash

# Run the inference
ros2 launch dynosam_ros dyno_sam_online_rgbd_launch.py \
    online:=true \
    input_image_mode:=1 \
    dataset_path:=/root/data \
    params_path:=/home/user/dev_ws/src/core/dynosam/params/ \
    output_path:=/root/results \
    name:=test

---

---- DepthAnythingV2
conda create -n depth-anything python=3.11 -y
conda activate depth-anything
python --version

conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y


git clone https://github.com/DepthAnything/Depth-Anything-V2.git
cd Depth-Anything-V2

pip install -U pip
pip install -r requirements.txt

conda remove -y pytorch torchvision torchaudio pytorch-cuda

pip3 install torch torchvision torchaudio

mkdir -p checkpoints
curl -L -o checkpoints/depth_anything_v2_vitb.pth \
  https://huggingface.co/depth-anything/Depth-Anything-V2-Base/resolve/main/depth_anything_v2_vitb.pth


python run.py \
  --encoder vitb \
  --img-path data/frames \
  --outdir data/depth \
  --pred-only \
  --grayscale


---- DepthAnythingV3
conda create -n da3 python=3.12 -y
conda activate da3
python -m pip install -U pip setuptools wheel
python -m pip install awesome-depth-anything-3

-
cat << 'EOF' > depth-anything
#!/usr/bin/env python3
import argparse, os, glob, torch
import numpy as np
from PIL import Image
from depth_anything_3.api import DepthAnything3

parser = argparse.ArgumentParser(description="Depth Anything 3: frames folder -> depth PNGs")
parser.add_argument("input", help="folder with frames (e.g. data/frames)")
parser.add_argument("-o", "--out", default="data/depth", help="output folder")
parser.add_argument("--model", default="depth-anything/DA3-SMALL", help="HF model id")
parser.add_argument("--device", default="auto", choices=["auto","cuda","cpu"], help="where to run")
parser.add_argument("--amp", action="store_true", help="use mixed precision on CUDA (saves VRAM)")
args = parser.parse_args()

os.environ.setdefault("PYTORCH_ALLOC_CONF", "expandable_segments:True")

os.makedirs(args.out, exist_ok=True)
paths = sorted([p for p in glob.glob(os.path.join(args.input, "*")) if os.path.isfile(p)])

if args.device == "auto":
    device = "cuda" if torch.cuda.is_available() else "cpu"
else:
    device = args.device

model = DepthAnything3.from_pretrained(args.model).to(device)
model.eval()

use_amp = args.amp and device == "cuda"
autocast = torch.cuda.amp.autocast if device == "cuda" else torch.cpu.amp.autocast

with torch.no_grad():
    for p in paths:
        # run 1 frame at a time to minimize VRAM
        if use_amp:
            with autocast(dtype=torch.float16):
                pred = model.inference([p])
        else:
            pred = model.inference([p])

        d = pred.depth[0]
        d = (d - d.min()) / (d.max() - d.min() + 1e-8)
        out = os.path.join(args.out, os.path.basename(p))
        Image.fromarray((d * 255).astype("uint8")).save(out)

        if device == "cuda":
            torch.cuda.empty_cache()

print(f"Saved depth maps to {args.out}")
EOF

chmod +x depth-anything

--

./depth-anything data/frames --model depth-anything/DA3-SMALL

---
./docker/create_container_base.sh dyno_sam_local dyno_sam2 \
  /path/on/host/data \
  /path/on/host/results \
  /path/on/host/DynoSAM \
  /path/on/host/third_parties



-----
# Fix OpenMPI error
# Install OpenMPI rootless

mkdir -p ~/openmpi-source
cd ~/openmpi-source

wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.6.tar.gz
tar -xvf openmpi-4.1.6.tar.gz
cd openmpi-4.1.6


source ~/.bashrc

cd ~/openmpi-source/openmpi-4.1.6

./configure --prefix=$HOME/openmpi \
            --with-cuda=/usr/local/cuda \
            --enable-mpi-cxx \
            --with-wrapper-cflags="-I/usr/local/cuda/include" \
            --with-wrapper-ldflags="-L/usr/local/cuda/lib64"

make -j$(nproc)
make install

which mpicc

export MPI_C_COMPILER=/home/huynh.ha.phuong.linh/openmpi/bin/mpicc

export MPI_CXX_COMPILER=/home/huynh.ha.phuong.linh/openmpi/bin/mpicxx

colcon build