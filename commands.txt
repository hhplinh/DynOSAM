cd dynosam_pkg/DynoSAM/docker &&
./build_docker_amd64.sh //


./create_container_amd64.sh //

docker exec -it dyno_sam bash

cd /home/user/dev_w &&
export MAKEFLAGS="-j10" && clear && colcon build


---- ros package
cd src

ros2 pkg create rgbd_publisher_pkg \
  --build-type ament_python \
  --dependencies rclpy sensor_msgs cv_bridge

cd ..

nano src rgbd_publisher_pkg/setup.py

--

from setuptools import setup

package_name = 'rgbd_publisher_pkg'

setup(
    name=package_name,
    version='0.0.0',
    packages=[package_name],
    install_requires=['setuptools'],
    zip_safe=True,
    maintainer='user',
    maintainer_email='user@todo.todo',
    description='RGBD publisher for DynoSAM',
    license='TODO',
    tests_require=['pytest'],
    entry_points={
        'console_scripts': [
            'rgbd_publisher = rgbd_publisher_pkg.rgbd_publisher:main',
        ],
    },
)
--
nano src/rgbd_publisher_pkg/rgbd_publisher_pkg/rgbd_publisher.py

--
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
import cv2
import glob
import yaml


class RGBDPublisher(Node):

    def __init__(self):
        super().__init__('rgbd_publisher')

        self.rgb_pub = self.create_publisher(
            Image,
            '/camera/color/image_rect_color',
            10)

        self.depth_pub = self.create_publisher(
            Image,
            '/camera/aligned_depth_to_color/image_raw',
            10)

        self.info_pub = self.create_publisher(
            CameraInfo,
            '/camera/color/camera_info',
            10)

        self.bridge = CvBridge()

        # CHANGE THESE PATHS IF NEEDED
        self.rgb_files = sorted(glob.glob('/root/data/rgb/*.png'))
        self.depth_files = sorted(glob.glob('/root/data/depth/*.png'))

        self.cam_info = self.load_camera_info('/root/data/camera_info.yaml')

        self.idx = 0
        self.timer = self.create_timer(0.033, self.publish_frame)

    def load_camera_info(self, path):
        msg = CameraInfo()
        with open(path) as f:
            data = yaml.safe_load(f)

        msg.width = data['image_width']
        msg.height = data['image_height']
        msg.k = data['K']
        msg.d = data['D']
        msg.r = data['R']
        msg.p = data['P']
        msg.distortion_model = data['distortion_model']

        return msg

    def publish_frame(self):
        if self.idx >= len(self.rgb_files):
            return

        rgb = cv2.imread(self.rgb_files[self.idx])
        depth = cv2.imread(self.depth_files[self.idx], -1)

        rgb_msg = self.bridge.cv2_to_imgmsg(rgb, encoding='bgr8')
        depth_msg = self.bridge.cv2_to_imgmsg(depth, encoding='16UC1')

        now = self.get_clock().now().to_msg()
        rgb_msg.header.stamp = now
        depth_msg.header.stamp = now
        self.cam_info.header.stamp = now

        self.rgb_pub.publish(rgb_msg)
        self.depth_pub.publish(depth_msg)
        self.info_pub.publish(self.cam_info)

        self.idx += 1


def main():
    rclpy.init()
    node = RGBDPublisher()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()


--

# in home/user/dev_ws
colcon build

source install/setup.bash

ros2 run rgbd_publisher_pkg rgbd_publisher


----

source install/setup.bash


ros2 launch dynosam_ros dyno_sam_launch.py \
  params_path:=/home/user/dev_ws/src/core/dynosam/params/ \
  dataset_path:=/root/data \
  v:=1 \
  data_provider_type:=3 \
  output_path:=/root/results




ros2 launch dynosam_ros dyno_sam_online_rgbd_launch.py \
    dataset_path:=/root/data \
    output_path:=/root/results \
    name:=test \
    data_provider_type:=3 \
    run_pipeline:=true


ros2 launch dynosam_ros dyno_sam_online_rgbd_launch.py \
    online:=true \
    input_image_mode:=1 \
    dataset_path:=/root/data \
    params_path:=/home/user/dev_ws/src/core/dynosam/params/ \
    output_path:=/root/results \
    name:=test

---

---- DepthAnythingV2
conda create -n depth-anything python=3.11 -y
conda activate depth-anything
python --version

conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y


git clone https://github.com/DepthAnything/Depth-Anything-V2.git
cd Depth-Anything-V2

pip install -U pip
pip install -r requirements.txt

conda remove -y pytorch torchvision torchaudio pytorch-cuda

pip3 install torch torchvision torchaudio

mkdir -p checkpoints
curl -L -o checkpoints/depth_anything_v2_vitb.pth \
  https://huggingface.co/depth-anything/Depth-Anything-V2-Base/resolve/main/depth_anything_v2_vitb.pth


python run.py \
  --encoder vitb \
  --img-path data/frames \
  --outdir data/depth \
  --pred-only \
  --grayscale


---- DepthAnythingV3
conda create -n da3 python=3.12 -y
conda activate da3
python -m pip install -U pip setuptools wheel
python -m pip install awesome-depth-anything-3

-
cat << 'EOF' > depth-anything
#!/usr/bin/env python3
import argparse, os, glob, torch
import numpy as np
from PIL import Image
from depth_anything_3.api import DepthAnything3

parser = argparse.ArgumentParser(description="Depth Anything 3: frames folder -> depth PNGs")
parser.add_argument("input", help="folder with frames (e.g. data/frames)")
parser.add_argument("-o", "--out", default="data/depth", help="output folder")
parser.add_argument("--model", default="depth-anything/DA3-SMALL", help="HF model id")
parser.add_argument("--device", default="auto", choices=["auto","cuda","cpu"], help="where to run")
parser.add_argument("--amp", action="store_true", help="use mixed precision on CUDA (saves VRAM)")
args = parser.parse_args()

os.environ.setdefault("PYTORCH_ALLOC_CONF", "expandable_segments:True")

os.makedirs(args.out, exist_ok=True)
paths = sorted([p for p in glob.glob(os.path.join(args.input, "*")) if os.path.isfile(p)])

if args.device == "auto":
    device = "cuda" if torch.cuda.is_available() else "cpu"
else:
    device = args.device

model = DepthAnything3.from_pretrained(args.model).to(device)
model.eval()

use_amp = args.amp and device == "cuda"
autocast = torch.cuda.amp.autocast if device == "cuda" else torch.cpu.amp.autocast

with torch.no_grad():
    for p in paths:
        # run 1 frame at a time to minimize VRAM
        if use_amp:
            with autocast(dtype=torch.float16):
                pred = model.inference([p])
        else:
            pred = model.inference([p])

        d = pred.depth[0]
        d = (d - d.min()) / (d.max() - d.min() + 1e-8)
        out = os.path.join(args.out, os.path.basename(p))
        Image.fromarray((d * 255).astype("uint8")).save(out)

        if device == "cuda":
            torch.cuda.empty_cache()

print(f"Saved depth maps to {args.out}")
EOF

chmod +x depth-anything

--

./depth-anything data/frames --model depth-anything/DA3-SMALL

